# AWS-Based ETL Pipeline Project

## Overview
This project demonstrates an end-to-end ETL pipeline using AWS services.

## AWS Services Used
- Amazon S3
- Amazon RDS (MySQL)
- AWS IAM

## Technologies
- Python
- Pandas
- Boto3
- SQLAlchemy

## Workflow
1. Extract raw data from S3
2. Transform data using Pandas
3. Store cleaned data in S3
4. Load structured data into RDS

## ðŸ“Š Project Outcome

- Successfully built an end-to-end ETL pipeline using AWS services.
- Automated extraction of raw sales data from Amazon S3.
- Cleaned and transformed data using Python (Pandas).
- Generated new calculated fields such as Total Amount.
- Stored cleaned data back into S3 for structured storage.
- Loaded processed data into Amazon RDS (MySQL) for analytical querying.
- Implemented secure access control using AWS IAM roles and policies.
- Improved data consistency and ensured schema standardization.

## ðŸ“ˆ Business Impact

- Enabled centralized storage of raw and processed datasets in S3.
- Improved data quality by removing null values and standardizing columns.
- Reduced manual data handling through automated ETL workflow.
- Enabled structured querying and reporting using RDS MySQL.
- Followed cloud security best practices using IAM access control.
